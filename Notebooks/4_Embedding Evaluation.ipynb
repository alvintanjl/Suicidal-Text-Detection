{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbMvtpz4-5km","executionInfo":{"status":"ok","timestamp":1667898277335,"user_tz":-480,"elapsed":22880,"user":{"displayName":"Alvin Tan","userId":"10916206877093889103"}},"outputId":"a1bc3efd-cc1b-4a5a-f6fe-3c391efaafa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","/content/gdrive/.shortcut-targets-by-id/1dXwjUxZF5kudup3owQmdj8MrIytqxvzx/BT4222 Project Group/Codes\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive/', force_remount=True)\n","# %cd gdrive/MyDrive/BT4222 Project Group/Codes/"]},{"cell_type":"code","source":["# imports\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.metrics import accuracy_score, f1_score\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import random\n","from keras import backend as K"],"metadata":{"id":"nPOB8QfOC2PF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# deep learning model classes\n","\n","class LSTM:\n","  def __init__(self):\n","    self.model = None\n","\n","  def fit(self, X_train, y_train):\n","    model = keras.Sequential([\n","        layers.Input(shape=(X_train.shape[1],1)),\n","        layers.SpatialDropout1D(0.2),\n","        layers.LSTM(200, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n","        layers.LSTM(200, recurrent_dropout=0.2, return_sequences=True),\n","        layers.Dense(300, activation='relu'),\n","        layers.Dense(300, activation='relu'),\n","        layers.Dense(1, activation='sigmoid')\n","      ])\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m])\n","    model.fit(X_train, y_train, batch_size=200, epochs=6, verbose=0)\n","    self.model = model\n","\n","  def predict(self, X_test):\n","    return self.model.predict(X_test)\n","\n","  def evaluate(self, X_test, y_test):\n","    return self.model.evaluate(X_test, y_test, verbose=0)\n","\n","class CNN:\n","  def __init__(self):\n","    self.model = None\n","\n","  def fit(self, X_train, y_train):\n","    model = keras.Sequential([\n","        layers.Input(shape=(X_train.shape[1],1)),\n","        layers.Conv1D(filters=128, kernel_size=5, strides=1, activation='relu', padding='same'),\n","        layers.GlobalMaxPooling1D(),\n","        layers.Dense(500, activation='relu'), # FCNN\n","        layers.Dropout(0.3),\n","        layers.Dense(500, activation='relu'),\n","        layers.Dropout(0.3),\n","        layers.Dense(1, activation='sigmoid', name = 'Output') # output\n","      ])\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m])\n","    model.fit(X_train, y_train, batch_size=200, epochs=6, verbose=0)\n","    self.model = model\n","\n","  def predict(self, X_test):\n","    return self.model.predict(X_test)\n","\n","  def evaluate(self, X_test, y_test):\n","    return self.model.evaluate(X_test, y_test, verbose=0)"],"metadata":{"id":"qzj854ogDuig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# common functions\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","def embeddings_evaluation(all_embeddings, model):\n","  score_df = pd.DataFrame()\n","  for name, embedding in all_embeddings.items():\n","    if name == 'TF-IDF w/ Bigram' and (type(model) is LSTM or type(model) is CNN):\n","      continue\n","    try:\n","      X_train = embedding['X_train']\n","      y_train = embedding['y_train']\n","      X_test = embedding['X_test']\n","      y_test = embedding['y_test']\n","      model.fit(X_train, y_train)\n","      y_pred = model.predict(X_test)\n","      acc = accuracy_score(y_test, y_pred)\n","      f1 = f1_score(y_test, y_pred, average='weighted')\n","    except ValueError as e:\n","      scores = model.evaluate(X_test, y_test)\n","      acc = scores[1]\n","      f1 = scores[2]\n","    except Exception as e:\n","      print(e)\n","    finally:\n","      score_df = score_df.append({'Embedding': name, 'Accuracy': acc, 'F1-score': f1}, ignore_index = True)\n","  return score_df"],"metadata":{"id":"rN1AsVLuF_Qa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdn_index = random.sample(range(110248), 40000)"],"metadata":{"id":"xKoyH6Catp-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# word2vec: skip-gram\n","X_train_sg = pd.read_csv('../Word Embedding/emb_sg_train.csv').iloc[rdn_index]\n","X_test_sg = pd.read_csv('../Word Embedding/emb_sg_test.csv')\n","\n","# word2vec: cbow\n","X_train_cbow = pd.read_csv('../Word Embedding/emb_cbow_train.csv').iloc[rdn_index]\n","X_test_cbow = pd.read_csv('../Word Embedding/emb_cbow_test.csv')\n","\n","# doc2vec: dbow\n","X_train_dbow = pd.read_csv('../Word Embedding/emb_dbow_train.csv').iloc[rdn_index]\n","X_test_dbow = pd.read_csv('../Word Embedding/emb_dbow_test.csv')\n","\n","# doc2vec: dm\n","X_train_dm = pd.read_csv('../Word Embedding/emb_dm_train.csv').iloc[rdn_index]\n","X_test_dm = pd.read_csv('../Word Embedding/emb_dm_test.csv')\n","\n","# doc2vec: dbow + dm\n","X_train_dbow_dm = pd.read_csv('../Word Embedding/emb_dbow_dm_train.csv').iloc[rdn_index]\n","X_test_dbow_dm = pd.read_csv('../Word Embedding/emb_dbow_dm_test.csv')\n","\n","# google's word2vec\n","X_train_ggl = pd.read_csv('../Word Embedding/emb_ggl_train.csv').iloc[rdn_index]\n","X_test_ggl = pd.read_csv('../Word Embedding/emb_ggl_test.csv')\n","\n","# standford's glove\n","X_train_glove = pd.read_csv('../Word Embedding/emb_glove_train.csv').iloc[rdn_index]\n","X_test_glove = pd.read_csv('../Word Embedding/emb_glove_test.csv')\n","\n","# bert distilled\n","X_train_bert = pd.read_csv('../Word Embedding/emb_bert_train.csv').iloc[rdn_index]\n","X_test_bert = pd.read_csv('../Word Embedding/emb_bert_test.csv')\n","\n","# tf-idf with bigram\n","train_untokenized_posts = pd.read_csv('../Data/X_train.csv')['processed_str'].to_numpy()\n","test_untokenized_posts = pd.read_csv('../Data/X_test.csv')['processed_str'].to_numpy()\n","vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n","X_train_tfidf = vectorizer.fit_transform(train_untokenized_posts)[rdn_index]\n","X_test_tfidf = vectorizer.transform(test_untokenized_posts)\n","\n","# labels\n","y_train = pd.read_csv('./Data/y_train.csv')['class'].iloc[rdn_index]\n","y_test = pd.read_csv('./Data/y_test.csv')['class']"],"metadata":{"id":"eR0bthSJCQl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all embeddings\n","embedding_dict = {\n","    'Word2Vec: Skip-Gram' : {'X_train': X_train_sg, 'y_train': y_train, 'X_test': X_test_sg, 'y_test': y_test},\n","    'Word2Vec: CBOW' : {'X_train': X_train_cbow, 'y_train': y_train, 'X_test': X_test_cbow, 'y_test': y_test},\n","    'Doc2Vec: DBOW' : {'X_train': X_train_dbow, 'y_train': y_train, 'X_test': X_test_dbow, 'y_test': y_test},\n","    'Doc2Vec: DM' : {'X_train': X_train_dm, 'y_train': y_train, 'X_test': X_test_dm, 'y_test': y_test},\n","    'Doc2Vec: DBOW+DM' : {'X_train': X_train_dbow_dm, 'y_train': y_train, 'X_test': X_test_dbow_dm, 'y_test': y_test},\n","    \"Google's Word2Vec\": {'X_train': X_train_ggl, 'y_train': y_train, 'X_test': X_test_ggl, 'y_test': y_test},\n","    'GloVe': {'X_train': X_train_glove, 'y_train': y_train, 'X_test': X_test_glove, 'y_test': y_test},\n","    'Bert-Distilled': {'X_train': X_train_bert, 'y_train': y_train, 'X_test': X_test_bert, 'y_test': y_test},\n","    'TF-IDF w/ Bigram': {'X_train': X_train_tfidf, 'y_train': y_train, 'X_test': X_test_tfidf, 'y_test': y_test}\n","}"],"metadata":{"id":"axFgc_ahJ-ud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_to_evaluate = {\n","    'LSTM': LSTM(),\n","    'CNN': CNN(),\n","    'Random Forest': RandomForestClassifier(),\n","    'Logistic Regression': LogisticRegression(max_iter=10000),\n","    'SVM': SVC(),\n","  }"],"metadata":{"id":"KKvRDvGODjW0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation_result = {}\n","\n","for model_name, model in models_to_evaluate.items():\n","  scores = embeddings_evaluation(embedding_dict, model)\n","  print(model_name)\n","  print(scores, '\\n')\n","  evaluation_result[model_name] = scores"],"metadata":{"id":"QMLC0fAuPAr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# write the result into a file\n","with open('Embedding Results.txt', 'a') as the_file:\n","  for model_name, scores in evaluation_result.items():\n","    the_file.write(f'{model_name}\\n')\n","    the_file.write(f'{scores}\\n\\n')"],"metadata":{"id":"mPbA_GRn4Mw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# After transferring scores into excel sheet, read it in\n","import pandas as pd\n","embedding_scores_df = pd.read_excel('../Embedding Results.xlsx', header = [0,1])\n","embedding_scores_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"nYW_HY71epb9","executionInfo":{"status":"ok","timestamp":1667898283514,"user_tz":-480,"elapsed":1776,"user":{"displayName":"Alvin Tan","userId":"10916206877093889103"}},"outputId":"f7dc8075-142a-4eb1-d445-0acfe7ed2d9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                  Model       LSTM                  CNN            \\\n","                Metrics   Accuracy  F1-Score   Accuracy  F1-Score   \n","0  Word2Vec: Skip-Gram   0.847859   0.847525  0.874537   0.826418   \n","1       Word2Vec: CBOW   0.865441   0.848034  0.866338   0.821691   \n","2        Doc2Vec: DBOW   0.607503   0.000293  0.607612   0.000841   \n","3          Doc2Vec: DM   0.651267   0.661997  0.677636   0.543574   \n","4     Doc2Vec: DBOW+DM   0.635850   0.803589  0.607612   0.000841   \n","5    Google's Word2Vec   0.708747   0.803620  0.816523   0.763112   \n","6               GloVe    0.798756   0.703097  0.780785   0.729949   \n","7      Bert-Distilled    0.607503   0.810928  0.763950   0.608729   \n","8      TF-IDF w/ Bigram        NaN       NaN        NaN       NaN   \n","\n","  Random Forest           Logistic Regression                  SVM            \n","       Accuracy  F1-Score            Accuracy  F1-Score   Accuracy  F1-Score  \n","0     0.901277   0.900677           0.908388   0.907801  0.882991   0.880982  \n","1     0.900551   0.900214           0.906719   0.906069  0.902438   0.901698  \n","2     0.669763   0.657508           0.631413   0.585238  0.607503   0.459172  \n","3     0.696575   0.685189           0.802010   0.793093  0.789892   0.787094  \n","4     0.707931   0.691726           0.805566   0.797999  0.778245   0.758210  \n","5     0.852224   0.849381           0.886002   0.884934  0.894021   0.893228  \n","6     0.842718   0.840331           0.854038   0.851934  0.856687   0.854773  \n","7     0.859988   0.858037           0.903635   0.903306  0.895399   0.894817  \n","8     0.858719   0.856053           0.851462   0.847206  0.856687   0.852750  "],"text/html":["\n","  <div id=\"df-e6902098-935c-4222-86a8-bbdc11d4d566\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th>Model</th>\n","      <th colspan=\"2\" halign=\"left\">LSTM</th>\n","      <th colspan=\"2\" halign=\"left\">CNN</th>\n","      <th colspan=\"2\" halign=\"left\">Random Forest</th>\n","      <th colspan=\"2\" halign=\"left\">Logistic Regression</th>\n","      <th colspan=\"2\" halign=\"left\">SVM</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>Metrics</th>\n","      <th>Accuracy</th>\n","      <th>F1-Score</th>\n","      <th>Accuracy</th>\n","      <th>F1-Score</th>\n","      <th>Accuracy</th>\n","      <th>F1-Score</th>\n","      <th>Accuracy</th>\n","      <th>F1-Score</th>\n","      <th>Accuracy</th>\n","      <th>F1-Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Word2Vec: Skip-Gram</td>\n","      <td>0.847859</td>\n","      <td>0.847525</td>\n","      <td>0.874537</td>\n","      <td>0.826418</td>\n","      <td>0.901277</td>\n","      <td>0.900677</td>\n","      <td>0.908388</td>\n","      <td>0.907801</td>\n","      <td>0.882991</td>\n","      <td>0.880982</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Word2Vec: CBOW</td>\n","      <td>0.865441</td>\n","      <td>0.848034</td>\n","      <td>0.866338</td>\n","      <td>0.821691</td>\n","      <td>0.900551</td>\n","      <td>0.900214</td>\n","      <td>0.906719</td>\n","      <td>0.906069</td>\n","      <td>0.902438</td>\n","      <td>0.901698</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Doc2Vec: DBOW</td>\n","      <td>0.607503</td>\n","      <td>0.000293</td>\n","      <td>0.607612</td>\n","      <td>0.000841</td>\n","      <td>0.669763</td>\n","      <td>0.657508</td>\n","      <td>0.631413</td>\n","      <td>0.585238</td>\n","      <td>0.607503</td>\n","      <td>0.459172</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Doc2Vec: DM</td>\n","      <td>0.651267</td>\n","      <td>0.661997</td>\n","      <td>0.677636</td>\n","      <td>0.543574</td>\n","      <td>0.696575</td>\n","      <td>0.685189</td>\n","      <td>0.802010</td>\n","      <td>0.793093</td>\n","      <td>0.789892</td>\n","      <td>0.787094</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Doc2Vec: DBOW+DM</td>\n","      <td>0.635850</td>\n","      <td>0.803589</td>\n","      <td>0.607612</td>\n","      <td>0.000841</td>\n","      <td>0.707931</td>\n","      <td>0.691726</td>\n","      <td>0.805566</td>\n","      <td>0.797999</td>\n","      <td>0.778245</td>\n","      <td>0.758210</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Google's Word2Vec</td>\n","      <td>0.708747</td>\n","      <td>0.803620</td>\n","      <td>0.816523</td>\n","      <td>0.763112</td>\n","      <td>0.852224</td>\n","      <td>0.849381</td>\n","      <td>0.886002</td>\n","      <td>0.884934</td>\n","      <td>0.894021</td>\n","      <td>0.893228</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>GloVe</td>\n","      <td>0.798756</td>\n","      <td>0.703097</td>\n","      <td>0.780785</td>\n","      <td>0.729949</td>\n","      <td>0.842718</td>\n","      <td>0.840331</td>\n","      <td>0.854038</td>\n","      <td>0.851934</td>\n","      <td>0.856687</td>\n","      <td>0.854773</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Bert-Distilled</td>\n","      <td>0.607503</td>\n","      <td>0.810928</td>\n","      <td>0.763950</td>\n","      <td>0.608729</td>\n","      <td>0.859988</td>\n","      <td>0.858037</td>\n","      <td>0.903635</td>\n","      <td>0.903306</td>\n","      <td>0.895399</td>\n","      <td>0.894817</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>TF-IDF w/ Bigram</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.858719</td>\n","      <td>0.856053</td>\n","      <td>0.851462</td>\n","      <td>0.847206</td>\n","      <td>0.856687</td>\n","      <td>0.852750</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6902098-935c-4222-86a8-bbdc11d4d566')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e6902098-935c-4222-86a8-bbdc11d4d566 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e6902098-935c-4222-86a8-bbdc11d4d566');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":[],"metadata":{"id":"YoUtSDjvfAj5"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}