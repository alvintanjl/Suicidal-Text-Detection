{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1pB_BLFTXGD","executionInfo":{"status":"ok","timestamp":1667835388027,"user_tz":-480,"elapsed":36081,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"aa0ec0d8-696c-477c-977e-1e1ac30fe5f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","/content/gdrive/MyDrive/BT4222 Project Group/Codes/Data/Final Data - Test train\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive/', force_remount=True)\n","# %cd gdrive/MyDrive/BT4222 Project Group/Codes/Data/Final Data - Test train"]},{"cell_type":"code","source":["!pip install testfixtures\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yQIWBgV_pD-J","executionInfo":{"status":"ok","timestamp":1667820418292,"user_tz":-480,"elapsed":9638,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"f0138095-07b0-44f3-bf2c-3a52ed7c9971"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: testfixtures in /usr/local/lib/python3.7/dist-packages (7.0.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"]}]},{"cell_type":"code","source":["# imports\n","\n","import pandas as pd\n","import numpy as np\n","from gensim.models import Word2Vec, KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","import multiprocessing\n","from tqdm import tqdm, trange\n","from sklearn import utils\n","from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"Nn5-hw7_VVSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# common functions\n","\n","def tokenize(preprocessed_text):\n","  return preprocessed_text.split()\n","\n","def avg_word2vec(model, model_vocabs, tokenized_posts, size):\n","  list_of_wv = [[model.wv[token] for token in post if token in model_vocabs] for post in tokenized_posts]\n","  list_of_avg_wv = []\n","  for wvs in list_of_wv:\n","    if len(wvs) > 0:\n","      list_of_avg_wv.append(wvs.mean(axis=0))\n","    else:\n","      list_of_avg_wv.append(np.zeros(size, dtype=float))\n","  return  np.array(list_of_avg_wv)\n","\n","def tagged_document(post):\n","  return TaggedDocument(words=post)\n","\n","def avg_doc2vec(model, tagged_docs):\n","  sents = tagged_docs.values\n","  targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n","  return np.array(regressors)\n","\n","def bert_dist_embed(posts, tokenizer, model):\n","  embedding_res = np.empty(shape=(0, 768))\n","  for batch_no in trange(0, len(posts), 100):\n","    tokenized = tokenizer(list(posts[batch_no:batch_no+100]), \n","                          padding = True, \n","                          truncation = True, \n","                          return_tensors = \"pt\")\n","    with torch.no_grad():\n","      hidden = model(**tokenized)\n","      \n","    batch = hidden.last_hidden_state[:,0,:].cpu().detach().numpy()\n","    embedding_res = np.append(embedding_res, batch, axis=0)\n","  return embedding_res"],"metadata":{"id":"ChOBE4I5WJ1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenized posts for training and testing\n","\n","train_posts = pd.read_csv('../Data/X_train.csv')['processed_str'].apply(tokenize).to_numpy()\n","test_posts = pd.read_csv('../Data/X_test.csv')['processed_str'].apply(tokenize).to_numpy()\n","train_labels = pd.read_csv('../Data/y_train.csv')['class'].to_numpy()\n","test_labels = pd.read_csv('../Data/y_test.csv')['class'].to_numpy()"],"metadata":{"id":"BcvbzmhHUEF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd ../.."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIXyZOFLcidz","executionInfo":{"status":"ok","timestamp":1667820422297,"user_tz":-480,"elapsed":831,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"83b8b23a-a886-4bc9-ec89-7eb52b08a569"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/BT4222 Project Group/Codes\n"]}]},{"cell_type":"markdown","source":["### 1. Trained Word2Vec\n","*   taking the sum of the word vectors of all words in a post\n","*   variations: Skip-Gram, CBOW"],"metadata":{"id":"zCeMY2TYYAmu"}},{"cell_type":"code","source":["# implement skip and cbow model\n","skip_gram_model = Word2Vec(train_posts, sg = 1, min_count=1)\n","cbow_model = Word2Vec(train_posts, sg = 0, min_count=1)\n","\n","print(skip_gram_model)\n","print(cbow_model)"],"metadata":{"id":"t5sUzYWZTvpG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667810781539,"user_tz":-480,"elapsed":106284,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"0e3366ff-6fb8-41a4-96ad-0801a1af9967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word2Vec(vocab=53100, size=100, alpha=0.025)\n","Word2Vec(vocab=53100, size=100, alpha=0.025)\n"]}]},{"cell_type":"code","source":["# word embedding using skip gram model\n","skipgram_wordpool = set(skip_gram_model.wv.index2word)\n","X_train_sg = avg_word2vec(skip_gram_model, skipgram_wordpool, train_posts, 100)\n","X_test_sg = avg_word2vec(skip_gram_model, skipgram_wordpool, test_posts, 100)\n","\n","\n","# word embedding using cbow model\n","cbow_wordpool = set(cbow_model.wv.index2word)\n","X_train_cbow = avg_word2vec(cbow_model, cbow_wordpool, train_posts, 100)\n","X_test_cbow = avg_word2vec(cbow_model, cbow_wordpool, test_posts, 100)"],"metadata":{"id":"l9jGRdM9ZI-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_sg.shape, X_test_sg.shape, X_train_cbow.shape, X_test_cbow.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSwJSG_8fUzM","executionInfo":{"status":"ok","timestamp":1667814506052,"user_tz":-480,"elapsed":555,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"1e0b7b79-577a-46f6-b7ac-b37399441490"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(110248, 100) (27562, 100) (110248, 100) (27562, 100)\n"]}]},{"cell_type":"code","source":["# export to csv\n","pd.DataFrame(X_train_sg).to_csv('../Word Embedding/emb_sg_train.csv', index=False)\n","pd.DataFrame(X_test_sg).to_csv('../Word Embedding/emb_sg_test.csv', index=False)\n","pd.DataFrame(X_train_cbow).to_csv('../Word Embedding/emb_cbow_train.csv', index=False)\n","pd.DataFrame(X_test_cbow).to_csv('../Word Embedding/emb_cbow_test.csv', index=False)"],"metadata":{"id":"aclHE5Yra1lP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Pretrained Google's Word2Vec"],"metadata":{"id":"-He7kz2nb6iz"}},{"cell_type":"code","source":["# implement google's word2vec\n","filename = '../Pretrained Embedding Models/GoogleNews-vectors-negative300.bin'\n","google_model = KeyedVectors.load_word2vec_format(filename, binary=True) # each word will be represented as a vector of 300 numbers\n","\n","print(google_model.vector_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_7i39rRb6Fq","executionInfo":{"status":"ok","timestamp":1667814389001,"user_tz":-480,"elapsed":64301,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"a5f25375-a329-4692-8a0f-e8632ce8757d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["300\n"]}]},{"cell_type":"code","source":["# word embedding using google's word2vec model\n","google_wordpool = set(google_model.index2word)\n","X_train_ggl = avg_word2vec(google_model, google_wordpool, train_posts, 300)\n","X_test_ggl = avg_word2vec(google_model, google_wordpool, test_posts, 300)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VWp66exmdHZP","executionInfo":{"status":"ok","timestamp":1667814020174,"user_tz":-480,"elapsed":15046,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"9c3293b6-bce7-477f-abc8-a5a8f25c6039"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  import sys\n"]}]},{"cell_type":"code","source":["print(X_train_ggl.shape, X_test_ggl.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ORnZ1mzfRNF","executionInfo":{"status":"ok","timestamp":1667814472010,"user_tz":-480,"elapsed":523,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"2b1a98e2-21e8-4c63-cd7c-844b11259624"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(110248, 300) (27562, 300)\n"]}]},{"cell_type":"code","source":["# export to csv\n","pd.DataFrame(X_train_ggl).to_csv('../Word Embedding/emb_ggl_train.csv', index=False)\n","pd.DataFrame(X_test_ggl).to_csv('.../Word Embedding/emb_ggl_test.csv', index=False)"],"metadata":{"id":"CZ-CaNNBdbcg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Stanford's GloVe"],"metadata":{"id":"3aK_YxIreQ6l"}},{"cell_type":"code","source":["# implement glove\n","glove_input_file_6B = '../Pretrained Embedding Models/glove.6B.100d.txt'\n","word2vec_output_file = '../Pretrained Embedding Models/glove.6B.100d.txt.word2vec'\n","glove2word2vec(glove_input_file_6B, word2vec_output_file)\n","glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n","\n","print(glove_model.vector_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJhhSbAGeTBW","executionInfo":{"status":"ok","timestamp":1667814413148,"user_tz":-480,"elapsed":22483,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"c3484649-f807-4181-a749-710668b4561c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100\n"]}]},{"cell_type":"code","source":["# word embedding using glove\n","glove_wordpool = set(glove_model.index2word)\n","X_train_glove = avg_word2vec(glove_model, glove_wordpool, train_posts, 100)\n","X_test_glove = avg_word2vec(glove_model, glove_wordpool, test_posts, 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D21DBfaZfGUe","executionInfo":{"status":"ok","timestamp":1667814426423,"user_tz":-480,"elapsed":13279,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"574bf3a3-0574-48f4-9af9-40cec6403f09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  import sys\n"]}]},{"cell_type":"code","source":["print(X_train_glove.shape, X_test_glove.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"assMXD8ofEzJ","executionInfo":{"status":"ok","timestamp":1667814440640,"user_tz":-480,"elapsed":455,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"e3923362-45c3-455c-b4e5-8f2fcf21d767"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(110248, 100) (27562, 100)\n"]}]},{"cell_type":"code","source":["# export to csv\n","pd.DataFrame(X_train_glove).to_csv('../Word Embedding/emb_glove_train.csv', index=False)\n","pd.DataFrame(X_test_glove).to_csv('../Word Embedding/emb_glove_test.csv', index=False)"],"metadata":{"id":"OEjcBo20iaTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Trained Doc2Vec\n","*   Doc2vec is an NLP tool for representing documents as a vector and is a generalizing of the word2vec method\n","\n"],"metadata":{"id":"1J8pKKeDindO"}},{"cell_type":"code","source":["# represent each sentence as a TaggedDocument containing 2 parameters, words=tokenized_sentence and tag=label\n","train = pd.DataFrame({'label': train_labels, 'post': list(train_posts)}, columns=['label', 'post'])\n","test = pd.DataFrame({'label': test_labels, 'post': list(test_posts)}, columns=['label', 'post'])\n","train_tagged = train.apply(lambda r: TaggedDocument(words=r['post'], tags=[r['label']]), axis=1)\n","test_tagged = test.apply(lambda r: TaggedDocument(words=r['post'], tags=[r['label']]), axis=1)\n","\n","# visualize a TaggedDocument\n","train_tagged[0]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f4pNwbXinHL","executionInfo":{"status":"ok","timestamp":1667814536785,"user_tz":-480,"elapsed":2009,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"c325976c-3a22-4622-a7fc-875ed925fdfe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TaggedDocument(words=['donna', 'dress', 'cut', 'waldo', 'next', 'halloween', 'maybe', 'time', 'hell', 'able', 'find', 'gilt'], tags=[0])"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["# use multiple cores\n","cores = multiprocessing.cpu_count()\n","\n","# implement Distributed Bag of Words (DBOW) (similar concept to skip-gram)\n","model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n","model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n","for epoch in range(30):\n","    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n","    model_dbow.alpha -= 0.002\n","    model_dbow.min_alpha = model_dbow.alpha\n","\n","# implement Distributed Memory (DM) (similar concept to CBOW)\n","model_dm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n","model_dm.build_vocab([x for x in tqdm(train_tagged.values)])\n","for epoch in range(30):\n","    model_dm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n","    model_dm.alpha -= 0.002\n","    model_dm.min_alpha = model_dm.alpha"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hmJ7bdmm4O8","executionInfo":{"status":"ok","timestamp":1667815357877,"user_tz":-480,"elapsed":819829,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"6d6d2488-6c97-481e-f291-7acfdae4c4b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 110248/110248 [00:00<00:00, 3230973.02it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2914071.62it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2619788.49it/s]\n","WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n","100%|██████████| 110248/110248 [00:00<00:00, 2806878.76it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3049799.35it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2983448.46it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2895931.33it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2901382.43it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2386971.30it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2987361.12it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3049095.50it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2909432.90it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3414839.25it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2632840.23it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2755233.70it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2922174.30it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2993472.22it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2874616.14it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2886405.00it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2981274.92it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2964703.04it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2354136.56it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2738054.68it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2983044.29it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3301563.11it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2944822.05it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3002978.39it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2987785.77it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 1950314.12it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2775641.98it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2901036.58it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3073782.07it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3230860.14it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2952851.73it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2956457.65it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2894354.34it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2877012.74it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2961399.37it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2983121.27it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2862338.38it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3326860.35it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3011917.22it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2937432.92it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3199917.15it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2995430.73it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2815783.68it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2611458.84it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2728957.30it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2997508.38it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2982717.18it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2804342.40it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2778160.05it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2966928.62it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3090277.19it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2892833.36it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2871652.75it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2196833.25it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2917215.27it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3211339.55it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2943172.65it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 3031683.75it/s]\n","100%|██████████| 110248/110248 [00:00<00:00, 2957365.23it/s]\n"]}]},{"cell_type":"code","source":["# word embedding using dbow\n","X_train_dbow = avg_doc2vec(model_dbow, train_tagged)\n","X_test_dbow = avg_doc2vec(model_dbow, test_tagged)\n","\n","# word embedding using dm\n","X_train_dm = avg_doc2vec(model_dm, train_tagged)\n","X_test_dm = avg_doc2vec(model_dm, test_tagged)\n","\n","# word embedding by combining a paragraph vector from DBOW and DM to improve performance\n","model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n","model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n","model_dbow_dm = ConcatenatedDoc2Vec([model_dbow, model_dm])\n","X_train_dbow_dm = avg_doc2vec(model_dbow_dm, train_tagged)\n","X_test_dbow_dm = avg_doc2vec(model_dbow_dm, test_tagged)"],"metadata":{"id":"F-aRW-QUnR8g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_dbow.shape, X_test_dbow.shape, X_train_dm.shape, X_test_dm.shape, X_train_dbow_dm.shape, X_test_dbow_dm.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9GM425xfnAP","executionInfo":{"status":"ok","timestamp":1667818669169,"user_tz":-480,"elapsed":18,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"19b885e1-2582-4eae-cb2d-2dcc59d3609a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(110248, 300) (27562, 300) (110248, 300) (27562, 300) (110248, 600) (27562, 600)\n"]}]},{"cell_type":"code","source":["# export to csv\n","pd.DataFrame(X_train_dbow).to_csv('../Word Embedding/emb_dbow_train.csv', index=False)\n","pd.DataFrame(X_test_dbow).to_csv('../Word Embedding/emb_dbow_test.csv', index=False)\n","pd.DataFrame(X_train_dm).to_csv('../Word Embedding/emb_dm_train.csv', index=False)\n","pd.DataFrame(X_test_dm).to_csv('../Word Embedding/emb_dm_test.csv', index=False)\n","pd.DataFrame(X_train_dbow_dm).to_csv('../Word Embedding/emb_dbow_dm_train.csv', index=False)\n","pd.DataFrame(X_test_dbow_dm).to_csv('../Word Embedding/emb_dbow_dm_test.csv', index=False)"],"metadata":{"id":"x7T1EqyXp1qg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Embedding with BERT pre-trained (BERT Distilled)"],"metadata":{"id":"UcPWCyLhxVNP"}},{"cell_type":"code","source":["train_untokenized_posts = pd.read_csv('../Data/X_train.csv')['processed_str'].to_numpy()\n","test_untokenized_posts = pd.read_csv('../Data/X_test.csv')['processed_str'].to_numpy()"],"metadata":{"id":"YZ8KzBzazujJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# specify GPU device\n","device = torch.device(\"cuda\")\n","\n","# implement bert distilled model\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","bert_dist_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfRWu-jIxUtw","executionInfo":{"status":"ok","timestamp":1667753503113,"user_tz":-480,"elapsed":2322,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"7e65eafe-2fe7-4348-b7c4-fa4ca3bb2f73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# word embedding using bert distilled model\n","X_train_bert = bert_dist_embed(train_untokenized_posts, bert_tokenizer, bert_dist_model)\n","X_test_bert = bert_dist_embed(test_untokenized_posts, bert_tokenizer, bert_dist_model)"],"metadata":{"id":"5LfRYT2x18oi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_bert.shape, X_test_bert.shape)"],"metadata":{"id":"kRUvDV1Ehh9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# export to csv\n","pd.DataFrame(X_train_bert).to_csv('../Word Embedding/emb_bert_train.csv', index=False)\n","pd.DataFrame(X_test_bert).to_csv('../Word Embedding/emb_bert_test.csv', index=False)"],"metadata":{"id":"672sBk_I2-Ow"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. TF-IDF with Bigram"],"metadata":{"id":"D0gIuPVW3oZi"}},{"cell_type":"code","source":["train_untokenized_posts = pd.read_csv('../Data/X_train.csv')['processed_str'].to_numpy()\n","test_untokenized_posts = pd.read_csv('../Data/X_test.csv')['processed_str'].to_numpy()"],"metadata":{"id":"EN20vqVB8mKb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n","X_train_tfidf = vectorizer.fit_transform(train_untokenized_posts)\n","X_test_tfidf = vectorizer.transform(test_untokenized_posts)"],"metadata":{"id":"4x9CDUQk3gfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_tfidf.shape, X_test_tfidf.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4CWtDbRihmbg","executionInfo":{"status":"ok","timestamp":1667820457620,"user_tz":-480,"elapsed":11,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"f1f05468-44f5-4107-9bd6-fcf05aeb8e44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(110248, 947656) (27562, 337260)\n"]}]},{"cell_type":"code","source":["# export to csv\n","pd.DataFrame(X_train_tfidf).to_csv('../Word Embedding/emb_tfidf_train.csv', index=False)\n","pd.DataFrame(X_train_tfidf).to_csv('.,/Word Embedding/emb_tfidf_test.csv', index=False)"],"metadata":{"id":"Z1hKK51R6wR7"},"execution_count":null,"outputs":[]}]}