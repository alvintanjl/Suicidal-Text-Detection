{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/gdrive/', force_remount=True)\n","# %cd gdrive/MyDrive/BT4222 Project Group/Codes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfTi_hXVEj8_","executionInfo":{"status":"ok","timestamp":1668036859197,"user_tz":-480,"elapsed":4395,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"b3a010f8-939e-444c-ee2a-ee11db773a20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","/content/gdrive/MyDrive/BT4222 Project Group/Codes\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_fXttXEPEXq_","executionInfo":{"status":"ok","timestamp":1668036880312,"user_tz":-480,"elapsed":8731,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"8ab9958c-6275-469f-e34f-3a807c27cec6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertForSequenceClassification\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm, trange\n","from sklearn.metrics import accuracy_score\n","import random\n","import pickle\n","from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score"],"metadata":{"id":"BXxRk5RlESOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdn_index = random.sample(range(110248), 50000)"],"metadata":{"id":"kz_F2ZFaGdN8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_posts = pd.read_csv('./Data/Final Data - Test train/X_train.csv')['processed_str'].iloc[rdn_index]\n","test_posts = pd.read_csv('./Data/Final Data - Test train/X_test.csv')['processed_str']\n","train_labels = pd.read_csv('./Data/Final Data - Test train/y_train.csv')['class'].iloc[rdn_index]\n","test_labels = pd.read_csv('./Data/Final Data - Test train/y_test.csv')['class']"],"metadata":{"id":"WuL1cgVtE1jd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6us2a5VDsWi"},"outputs":[],"source":["# Use BertTokenizer to:\n","#   - Add special tokens\n","#   - Pad and make sentences same length\n","#   - Create attention mask\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","def preprocessing(input_text, tokenizer):\n","  '''\n","  Returns:\n","    - input_ids: list of token ids\n","    - attention_mask: list of indices (0,1) specifying which tokens should be considered by model\n","  '''\n","  encoding_dict = tokenizer.encode_plus(\n","      input_text,\n","      add_special_tokens = True,\n","      max_length = 155,\n","      pad_to_max_length = True,\n","      return_attention_mask = True,\n","      return_tensors = \"pt\"\n","  )\n","\n","  return encoding_dict['input_ids'], encoding_dict['attention_mask']"]},{"cell_type":"code","source":["train_idx = []\n","train_a_m = []\n","\n","for text in train_posts.values:\n","  input_ids, attention_mask = preprocessing(text, tokenizer)\n","  train_idx.append(input_ids)\n","  train_a_m.append(attention_mask)\n","\n","train_idx = torch.cat(train_idx, dim = 0)\n","train_a_m = torch.cat(train_a_m, dim = 0)\n","train_lab = torch.tensor(train_labels.values)\n","\n","val_idx = []\n","val_a_m = []\n","\n","for text in test_posts.values:\n","  input_ids, attention_mask = preprocessing(text, tokenizer)\n","  val_idx.append(input_ids)\n","  val_a_m.append(attention_mask)\n","\n","val_idx = torch.cat(val_idx, dim = 0)\n","val_a_m = torch.cat(val_a_m, dim = 0)\n","val_lab = torch.tensor(test_labels.values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NAqvvYgD_ae","executionInfo":{"status":"ok","timestamp":1668013720350,"user_tz":-480,"elapsed":17596,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"1f54d706-02d1-4403-a9af-bb71f8a7ccd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Specify batch size\n","BATCH_SIZE = 200\n","\n","# Create train & validation sets\n","train_set = TensorDataset(\n","    train_idx,\n","    train_a_m,\n","    train_lab\n",")\n","\n","val_set = TensorDataset(\n","    val_idx, \n","    val_a_m,\n","    val_lab\n",")\n","\n","# Prepare Dataloader\n","train_dataloader = DataLoader(\n","    train_set,\n","    sampler = RandomSampler(train_set),\n","    batch_size = BATCH_SIZE\n",")\n","\n","val_dataloader = DataLoader(\n","    val_set,\n","    sampler = SequentialSampler(val_set),\n","    batch_size = BATCH_SIZE\n",")"],"metadata":{"id":"JFDjGWKMEA_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def b_tp(preds, labels):\n","  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n","  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n","\n","def b_fp(preds, labels):\n","  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n","  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n","\n","def b_tn(preds, labels):\n","  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n","  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n","\n","def b_fn(preds, labels):\n","  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n","  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n","\n","def b_metrics(preds, labels):\n","  '''\n","  Returns the following metrics:\n","    - accuracy    = (TP + TN) / N\n","    - precision   = TP / (TP + FP)\n","    - recall      = TP / (TP + FN)\n","    - specificity = TN / (TN + FP)\n","  '''\n","  preds = np.argmax(preds, axis = 1).flatten()\n","  labels = labels.flatten()\n","  tp = b_tp(preds, labels)\n","  tn = b_tn(preds, labels)\n","  fp = b_fp(preds, labels)\n","  fn = b_fn(preds, labels)\n","  b_accuracy = (tp + tn) / len(labels)\n","  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n","  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n","  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n","  return b_accuracy, b_precision, b_recall, b_specificity"],"metadata":{"id":"aw6Q2AceEC1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the BertForSequenceClassification model\n","model = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels = 2,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","\n","# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n","optimizer = torch.optim.AdamW(model.parameters(), \n","                              lr = 5e-5,\n","                              eps = 1e-08\n","                              )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvMaxotIEEQa","executionInfo":{"status":"ok","timestamp":1668013726840,"user_tz":-480,"elapsed":6499,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"9751ac67-1c78-463a-f5ac-6dd437bf2f22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')\n","\n","# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n","epochs = 2\n","\n","for epoch in range(epochs):\n","    \n","    print(f'Epoch: {epoch + 1}')\n","    # ========== Training ==========\n","    \n","    # Set model to training mode\n","    model.train()\n","    \n","    # Tracking variables\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","\n","    print(' -- Training')\n","    for step, batch in tqdm(enumerate(train_dataloader)):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        optimizer.zero_grad()\n","        # Forward pass\n","        train_output = model(b_input_ids, \n","                             token_type_ids = None, \n","                             attention_mask = b_input_mask, \n","                             labels = b_labels)\n","        # Backward pass\n","        train_output.loss.backward()\n","        optimizer.step()\n","        # Update tracking variables\n","        tr_loss += train_output.loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","    # ========== Validation ==========\n","\n","    print(' -- Validation')\n","\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Tracking variables \n","    val_accuracy = []\n","    val_precision = []\n","    val_recall = []\n","    val_specificity = []\n","\n","    for batch in tqdm(val_dataloader):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","          # Forward pass\n","          eval_output = model(b_input_ids, \n","                              token_type_ids = None, \n","                              attention_mask = b_input_mask)\n","        logits = eval_output.logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        # Calculate validation metrics\n","        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n","        val_accuracy.append(b_accuracy)\n","        # Update precision only when (tp + fp) !=0; ignore nan\n","        if b_precision != 'nan': val_precision.append(b_precision)\n","        # Update recall only when (tp + fn) !=0; ignore nan\n","        if b_recall != 'nan': val_recall.append(b_recall)\n","        # Update specificity only when (tn + fp) !=0; ignore nan\n","        if b_specificity != 'nan': val_specificity.append(b_specificity)\n","\n","    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n","    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n","    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n","    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n","    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n","\n","# save model\n","import pickle\n","\n","file_name = \"../Models/bert.pkl\"\n","pickle.dump(model, open(file_name, \"wb\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YuLQY--9EGxS","outputId":"bb4c4d83-2ac6-4d84-aa65-eaa23369fed7","executionInfo":{"status":"ok","timestamp":1668030662718,"user_tz":-480,"elapsed":16935885,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1\n"," -- Training\n"]},{"output_type":"stream","name":"stderr","text":["250it [2:05:24, 30.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":[" -- Validation\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [15:47<00:00,  9.48s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.3197\n","\t - Validation Accuracy: 0.9070\n","\t - Validation Precision: 0.8602\n","\t - Validation Recall: 0.9204\n","\t - Validation Specificity: 0.9011\n","\n","Epoch: 2\n"," -- Training\n"]},{"output_type":"stream","name":"stderr","text":["250it [2:05:04, 30.02s/it]\n"]},{"output_type":"stream","name":"stdout","text":[" -- Validation\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [15:55<00:00,  9.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.1686\n","\t - Validation Accuracy: 0.9175\n","\t - Validation Precision: 0.8773\n","\t - Validation Recall: 0.9230\n","\t - Validation Specificity: 0.9163\n","\n"]}]},{"cell_type":"code","source":["# load\n","file_name = \"../Models/bert.pkl\"\n","bert_loaded = pickle.load(open(file_name, \"rb\"))"],"metadata":{"id":"mFUbaExYUyh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_idx = []\n","test_a_m = []\n","\n","for text in test_posts.values:\n","  input_ids, attention_mask = preprocessing(text, tokenizer)\n","  test_idx.append(input_ids)\n","  test_a_m.append(attention_mask)\n","\n","test_idx = torch.cat(test_idx, dim = 0)\n","test_a_m = torch.cat(test_a_m, dim = 0)\n","test_lab = torch.tensor(test_labels.values)"],"metadata":{"id":"NE0g8wFiEIdZ","executionInfo":{"status":"ok","timestamp":1668036973800,"user_tz":-480,"elapsed":4766,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd24bca6-1212-47ef-ca12-9202699eb062"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Test Results\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","with torch.no_grad():\n","  output = bert_loaded(test_idx.to(device), token_type_ids = None, attention_mask = test_a_m.to(device))\n","  preds = np.argmax(output.logits.cpu().numpy(), axis = 1).flatten()"],"metadata":{"id":"HtvyOTE2EKH3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668038197233,"user_tz":-480,"elapsed":1113946,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"89a3a720-511b-4ad6-b128-4eda5743fa4f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9155"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# precision, f1 all here\n","\n","data = {\n","    'Model': ['BERT'],\n","    'Accuracy': [accuracy_score(test_lab, preds)],\n","    'F1-score': [f1_score(test_lab, preds)],\n","    'Precision': [precision_score(test_lab, preds)],\n","    'Recall': [recall_score(test_lab, preds)]\n","}\n","\n","\n","df = pd.DataFrame(data)\n"," \n","# append data frame to CSV file\n","df.to_csv('../Model_Evaluation.csv', mode='a', index=False, header=False)"],"metadata":{"id":"C8cEk5jPWEg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.read_csv('./Model_Evaluation.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"vurM5Qcj2L1N","executionInfo":{"status":"ok","timestamp":1668038627588,"user_tz":-480,"elapsed":388,"user":{"displayName":"Lim Jie Xi","userId":"14475987453127014335"}},"outputId":"86aa9379-2265-4132-f8ec-7193f2941045"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           Model  Accuracy  F1-score  Precision    Recall\n","0           LSTM  0.902220  0.902100   0.880611  0.868645\n","1        XGBoost  0.912271  0.912105   0.896376  0.877981\n","2       LightGBM  0.908388  0.908270   0.888795  0.876225\n","3            SVM  0.910602  0.910176   0.906007  0.861620\n","4   Logistic Reg  0.910239  0.909736   0.908859  0.857275\n","5  Random Forest  0.901930  0.901466   0.894277  0.850712\n","6            CNN  0.913432  0.913176   0.902367  0.874006\n","7           BERT  0.915500  0.893911   0.874693  0.913992"],"text/html":["\n","  <div id=\"df-9ab52286-29b3-4972-9473-f577f98ea0dc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy</th>\n","      <th>F1-score</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LSTM</td>\n","      <td>0.902220</td>\n","      <td>0.902100</td>\n","      <td>0.880611</td>\n","      <td>0.868645</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>XGBoost</td>\n","      <td>0.912271</td>\n","      <td>0.912105</td>\n","      <td>0.896376</td>\n","      <td>0.877981</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LightGBM</td>\n","      <td>0.908388</td>\n","      <td>0.908270</td>\n","      <td>0.888795</td>\n","      <td>0.876225</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SVM</td>\n","      <td>0.910602</td>\n","      <td>0.910176</td>\n","      <td>0.906007</td>\n","      <td>0.861620</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Logistic Reg</td>\n","      <td>0.910239</td>\n","      <td>0.909736</td>\n","      <td>0.908859</td>\n","      <td>0.857275</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Random Forest</td>\n","      <td>0.901930</td>\n","      <td>0.901466</td>\n","      <td>0.894277</td>\n","      <td>0.850712</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CNN</td>\n","      <td>0.913432</td>\n","      <td>0.913176</td>\n","      <td>0.902367</td>\n","      <td>0.874006</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>BERT</td>\n","      <td>0.915500</td>\n","      <td>0.893911</td>\n","      <td>0.874693</td>\n","      <td>0.913992</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ab52286-29b3-4972-9473-f577f98ea0dc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9ab52286-29b3-4972-9473-f577f98ea0dc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9ab52286-29b3-4972-9473-f577f98ea0dc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":27}]}]}